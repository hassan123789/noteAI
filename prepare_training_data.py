"""
note.com ã‚¿ã‚¤ãƒˆãƒ«å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
==========================================
Phase 2: ãƒ‡ãƒ¼ã‚¿åˆ†æ â†’ ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚° â†’ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

ç”Ÿæˆç‰©:
- training_data.jsonl: Fine-tuningç”¨ãƒ‡ãƒ¼ã‚¿
- data_report.txt: ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ
"""

import json
import re
from datetime import datetime

import pandas as pd

# ============================================================
# è¨­å®š
# ============================================================
INPUT_CSV = "note_power_data.csv"
OUTPUT_JSONL = "training_data.jsonl"
OUTPUT_REPORT = "data_report.txt"

# Power Scoreé–¾å€¤ï¼ˆã“ã‚Œä»¥ä¸Šã‚’ã€ŒæˆåŠŸã‚¿ã‚¤ãƒˆãƒ«ã€ã¨ã™ã‚‹ï¼‰
SUCCESS_THRESHOLD = 1.0  # 1.0ä»¥ä¸Š = ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ä»¥ä¸Šã®ã‚¹ã‚­ã‚’ç²å¾—

# é™¤å¤–æ¡ä»¶ï¼ˆç·©å’Œç‰ˆï¼šã‚¿ã‚¤ãƒˆãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ã«ä¸è¦ãªã‚‚ã®ã®ã¿ï¼‰
EXCLUDE_PATTERNS = [
    r"^ã‚µã‚¤ãƒˆãƒãƒƒãƒ—$",  # ç›®æ¬¡ç³»ã®ã¿
    r"^ãƒã‚¬ã‚¸ãƒ³",  # éè¨˜äº‹ç³»
    r"^ãŠã¯ã‚ˆã†æœãµã¿",  # å®šå‹é€£è¼‰ï¼ˆå†’é ­ã®ã¿ï¼‰
]

# æœ€ä½ã‚¹ã‚­æ•°ï¼ˆãƒã‚¤ã‚ºé™¤å»ï¼‰- ç·©å’Œ
MIN_LIKES = 10


# ============================================================
# ãƒ‡ãƒ¼ã‚¿åˆ†æé–¢æ•°
# ============================================================
def analyze_data(df: pd.DataFrame) -> dict:
    """ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°åˆ†æ"""
    stats = {
        "total_records": len(df),
        "unique_users": df["user_id"].nunique(),
        "power_score": {
            "mean": df["power_score"].mean(),
            "median": df["power_score"].median(),
            "std": df["power_score"].std(),
            "min": df["power_score"].min(),
            "max": df["power_score"].max(),
            "q25": df["power_score"].quantile(0.25),
            "q75": df["power_score"].quantile(0.75),
        },
        "likes": {
            "mean": df["likes"].mean(),
            "median": df["likes"].median(),
            "min": df["likes"].min(),
            "max": df["likes"].max(),
        },
        "followers_dist": df["followers"].describe().to_dict(),
        "top_users": df.groupby("user_id")["power_score"].mean().nlargest(10).to_dict(),
        "title_length": {
            "mean": df["title"].str.len().mean(),
            "min": df["title"].str.len().min(),
            "max": df["title"].str.len().max(),
        },
    }
    return stats


def extract_title_features(title: str) -> dict:
    """ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ç‰¹å¾´ã‚’æŠ½å‡º"""
    features = {
        "length": len(title),
        "has_brackets": bool(re.search(r"[ã€ã€‘ã€Œã€ã€ã€\[\]]", title)),
        "has_numbers": bool(re.search(r"\d+", title)),
        "has_emoji": bool(
            re.search(r"[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿ğŸš€-ğŸ›¿ğŸ‡¦-ğŸ‡¿âœ‚-â°ğŸ”€-ğŸ”¿ğŸ•-ğŸ•§ğŸ–-ğŸ—‘ğŸ¤-ğŸ§¿ğŸ©°-ğŸ«¶]", title)
        ),
        "has_question": "?" in title or "ï¼Ÿ" in title,
        "has_exclamation": "!" in title or "ï¼" in title,
        "has_pipe": "|" in title or "ï½œ" in title,
        "word_count": len(title.split()),
        "has_money_term": bool(re.search(r"(ç¨¼|ä¸‡å††|åç›Š|æœˆå|å‰¯æ¥­|åå…¥)", title)),
        "has_action_verb": bool(
            re.search(r"(ã‚„ã£ã¦ã¿ãŸ|ã—ã¦ã¿ãŸ|è©¦ã—ãŸ|å§‹ã‚|æŒ‘æˆ¦)", title)
        ),
    }
    return features


# ============================================================
# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
# ============================================================
def clean_data(df: pd.DataFrame) -> tuple[pd.DataFrame, dict]:
    """ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†"""
    original_count = len(df)
    removal_log = {"original": original_count, "steps": []}

    # Step 1: é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã«è©²å½“ã™ã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’é™¤å»
    pattern = "|".join(EXCLUDE_PATTERNS)
    mask = ~df["title"].str.contains(pattern, case=False, regex=True, na=False)
    removed = len(df) - mask.sum()
    df = df[mask].copy()
    removal_log["steps"].append(
        {"step": "é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³", "removed": removed, "remaining": len(df)}
    )

    # Step 2: æœ€ä½ã‚¹ã‚­æ•°æœªæº€ã‚’é™¤å»
    mask = df["likes"] >= MIN_LIKES
    removed = len(df) - mask.sum()
    df = df[mask].copy()
    removal_log["steps"].append(
        {
            "step": f"æœ€ä½ã‚¹ã‚­æ•°({MIN_LIKES}æœªæº€)",
            "removed": removed,
            "remaining": len(df),
        }
    )

    # Step 3: é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«é™¤å»
    before = len(df)
    df = df.drop_duplicates(subset=["title"])
    removed = before - len(df)
    removal_log["steps"].append(
        {"step": "é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«", "removed": removed, "remaining": len(df)}
    )

    # Step 4: æ¥µç«¯ãªå¤–ã‚Œå€¤ã®ç¢ºèªï¼ˆå‰Šé™¤ã¯ã—ãªã„ãŒè¨˜éŒ²ï¼‰
    high_outliers = df[df["power_score"] > 10]
    if len(high_outliers) > 0:
        removal_log["outliers"] = {
            "high_power_score": len(high_outliers),
            "samples": high_outliers["title"].head(5).tolist(),
        }

    removal_log["final"] = len(df)
    removal_log["removed_total"] = original_count - len(df)
    removal_log["retention_rate"] = len(df) / original_count * 100

    return df, removal_log


# ============================================================
# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
# ============================================================
def create_training_data(df: pd.DataFrame) -> list[dict]:
    """JSOLNãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
    training_data = []

    for _, row in df.iterrows():
        # æˆåŠŸ/å¤±æ•—ãƒ©ãƒ™ãƒ«
        is_success = row["power_score"] >= SUCCESS_THRESHOLD

        # ç‰¹å¾´æŠ½å‡º
        features = extract_title_features(row["title"])

        # å­¦ç¿’ç”¨ãƒ¬ã‚³ãƒ¼ãƒ‰
        record = {
            # ãƒ¡ã‚¿æƒ…å ±
            "id": f"{row['user_id']}_{hash(row['title']) % 10000:04d}",
            # å…¥åŠ›ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‰
            "title": row["title"],
            # ãƒ©ãƒ™ãƒ«
            "label": "success" if is_success else "normal",
            "power_score": round(row["power_score"], 4),
            # è£œåŠ©æƒ…å ±
            "likes": int(row["likes"]),
            "followers": int(row["followers"]),
            # ç‰¹å¾´
            "features": features,
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼ï¼ˆFine-tuningç”¨ï¼‰
            "prompt": f"ä»¥ä¸‹ã®æ¡ä»¶ã§noteè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚\nã‚¿ã‚¤ãƒˆãƒ«: {row['title']}\n\nè©•ä¾¡:",
            "completion": f" {'é«˜ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ' if is_success else 'æ¨™æº–'}ï¼ˆã‚¹ã‚³ã‚¢: {row['power_score']:.2f}ï¼‰",
        }

        training_data.append(record)

    return training_data


def create_title_generation_data(df: pd.DataFrame) -> list[dict]:
    """ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆæˆåŠŸä¾‹ã®ã¿ï¼‰"""
    success_df = df[df["power_score"] >= SUCCESS_THRESHOLD].copy()
    generation_data = []

    for _, row in success_df.iterrows():
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        title = row["title"]

        # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¨æ¸¬
        keywords = []
        if re.search(r"å‰¯æ¥­|ç¨¼ã|åç›Š", title):
            keywords.append("å‰¯æ¥­ãƒ»åç›Šç³»")
        if re.search(r"AI|ChatGPT|Sora|ç”Ÿæˆ", title):
            keywords.append("AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼")
        if re.search(r"å­è‚²ã¦|è‚²å…|ãƒãƒ|ãƒ‘ãƒ‘", title):
            keywords.append("è‚²å…ãƒ»å®¶æ—")
        if re.search(r"è‡ªåˆ†|äººç”Ÿ|ç”Ÿã", title):
            keywords.append("è‡ªå·±å•“ç™º")
        if not keywords:
            keywords.append("ãã®ä»–")

        record = {
            "instruction": f"ã€Œ{', '.join(keywords)}ã€ã«é–¢ã™ã‚‹ã€èª­è€…ã®èˆˆå‘³ã‚’å¼•ãnoteè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚",
            "input": "",
            "output": title,
            "power_score": round(row["power_score"], 4),
            "likes": int(row["likes"]),
        }

        generation_data.append(record)

    return generation_data


# ============================================================
# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
# ============================================================
def generate_report(
    stats: dict, removal_log: dict, training_data: list, generation_data: list
) -> str:
    """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    report = []
    report.append("=" * 60)
    report.append("note.com ã‚¿ã‚¤ãƒˆãƒ«å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ - å“è³ªãƒ¬ãƒãƒ¼ãƒˆ")
    report.append(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("=" * 60)

    report.append("\n## 1. å…ƒãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ")
    report.append(f"- ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {stats['total_records']}")
    report.append(f"- ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {stats['unique_users']}")
    report.append(
        f"- Power Score: å¹³å‡ {stats['power_score']['mean']:.2f}, ä¸­å¤®å€¤ {stats['power_score']['median']:.2f}"
    )
    report.append(f"  - æœ€å°: {stats['power_score']['min']:.4f}")
    report.append(f"  - æœ€å¤§: {stats['power_score']['max']:.2f}")
    report.append(f"  - 25%ç‚¹: {stats['power_score']['q25']:.2f}")
    report.append(f"  - 75%ç‚¹: {stats['power_score']['q75']:.2f}")

    report.append("\n## 2. ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°çµæœ")
    report.append(f"- å…ƒãƒ‡ãƒ¼ã‚¿: {removal_log['original']} ä»¶")
    for step in removal_log["steps"]:
        report.append(
            f"  - {step['step']}: -{step['removed']} â†’ æ®‹ã‚Š {step['remaining']} ä»¶"
        )
    report.append(f"- æœ€çµ‚ãƒ‡ãƒ¼ã‚¿: {removal_log['final']} ä»¶")
    report.append(f"- ä¿æŒç‡: {removal_log['retention_rate']:.1f}%")

    if "outliers" in removal_log:
        report.append(
            f"\n### å¤–ã‚Œå€¤ï¼ˆPower Score > 10ï¼‰: {removal_log['outliers']['high_power_score']} ä»¶"
        )
        for sample in removal_log["outliers"]["samples"]:
            report.append(f"  - {sample[:50]}...")

    report.append("\n## 3. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ")
    success_count = sum(1 for d in training_data if d["label"] == "success")
    normal_count = len(training_data) - success_count
    report.append(f"- è©•ä¾¡ãƒ¢ãƒ‡ãƒ«ç”¨: {len(training_data)} ä»¶")
    report.append(
        f"  - æˆåŠŸãƒ©ãƒ™ãƒ«: {success_count} ä»¶ ({success_count / len(training_data) * 100:.1f}%)"
    )
    report.append(
        f"  - é€šå¸¸ãƒ©ãƒ™ãƒ«: {normal_count} ä»¶ ({normal_count / len(training_data) * 100:.1f}%)"
    )
    report.append(f"- ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç”¨: {len(generation_data)} ä»¶ï¼ˆæˆåŠŸä¾‹ã®ã¿ï¼‰")

    report.append("\n## 4. ã‚¿ã‚¤ãƒˆãƒ«ç‰¹å¾´åˆ†æï¼ˆæˆåŠŸä¾‹ï¼‰")
    success_data = [d for d in training_data if d["label"] == "success"]
    if success_data:
        bracket_count = sum(1 for d in success_data if d["features"]["has_brackets"])
        number_count = sum(1 for d in success_data if d["features"]["has_numbers"])
        money_count = sum(1 for d in success_data if d["features"]["has_money_term"])
        question_count = sum(1 for d in success_data if d["features"]["has_question"])

        report.append(
            f"- ã€ã€‘ç­‰ã®æ‹¬å¼§ä½¿ç”¨: {bracket_count} ä»¶ ({bracket_count / len(success_data) * 100:.1f}%)"
        )
        report.append(
            f"- æ•°å­—ä½¿ç”¨: {number_count} ä»¶ ({number_count / len(success_data) * 100:.1f}%)"
        )
        report.append(
            f"- é‡‘éŠ­é–¢é€£ãƒ¯ãƒ¼ãƒ‰: {money_count} ä»¶ ({money_count / len(success_data) * 100:.1f}%)"
        )
        report.append(
            f"- ç–‘å•å½¢: {question_count} ä»¶ ({question_count / len(success_data) * 100:.1f}%)"
        )

        avg_length = sum(d["features"]["length"] for d in success_data) / len(
            success_data
        )
        report.append(f"- å¹³å‡æ–‡å­—æ•°: {avg_length:.1f} æ–‡å­—")

    report.append("\n## 5. Top 10 æˆåŠŸã‚¿ã‚¤ãƒˆãƒ«")
    sorted_data = sorted(training_data, key=lambda x: x["power_score"], reverse=True)[
        :10
    ]
    for i, d in enumerate(sorted_data, 1):
        report.append(f"{i}. [{d['power_score']:.2f}] {d['title'][:50]}...")

    report.append("\n" + "=" * 60)
    report.append("ãƒ¬ãƒãƒ¼ãƒˆçµ‚äº†")
    report.append("=" * 60)

    return "\n".join(report)


# ============================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================================================
def main():
    print("=" * 60)
    print("Phase 2: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™é–‹å§‹")
    print("=" * 60)

    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\n[1/5] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    df = pd.read_csv(INPUT_CSV)
    print(f"  â†’ {len(df)} ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿")

    # 2. ãƒ‡ãƒ¼ã‚¿åˆ†æ
    print("\n[2/5] ãƒ‡ãƒ¼ã‚¿åˆ†æä¸­...")
    stats = analyze_data(df)
    print(
        f"  â†’ Power Score: å¹³å‡ {stats['power_score']['mean']:.2f}, ä¸­å¤®å€¤ {stats['power_score']['median']:.2f}"
    )

    # 3. ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    print("\n[3/5] ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ä¸­...")
    cleaned_df, removal_log = clean_data(df)
    print(
        f"  â†’ {removal_log['original']} â†’ {removal_log['final']} ä»¶ (ä¿æŒç‡: {removal_log['retention_rate']:.1f}%)"
    )

    # 4. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    print("\n[4/5] å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
    training_data = create_training_data(cleaned_df)
    generation_data = create_title_generation_data(cleaned_df)

    success_count = sum(1 for d in training_data if d["label"] == "success")
    print(
        f"  â†’ è©•ä¾¡ç”¨: {len(training_data)} ä»¶ (æˆåŠŸ: {success_count}, é€šå¸¸: {len(training_data) - success_count})"
    )
    print(f"  â†’ ç”Ÿæˆç”¨: {len(generation_data)} ä»¶")

    # 5. ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    print("\n[5/5] ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ä¸­...")

    # JSONLå‡ºåŠ›ï¼ˆè©•ä¾¡ãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰
    with open(OUTPUT_JSONL, "w", encoding="utf-8") as f:
        for record in training_data:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")
    print(f"  â†’ {OUTPUT_JSONL} ã‚’å‡ºåŠ›")

    # JSONLå‡ºåŠ›ï¼ˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰
    generation_jsonl = "generation_training.jsonl"
    with open(generation_jsonl, "w", encoding="utf-8") as f:
        for record in generation_data:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")
    print(f"  â†’ {generation_jsonl} ã‚’å‡ºåŠ›")

    # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    report = generate_report(stats, removal_log, training_data, generation_data)
    with open(OUTPUT_REPORT, "w", encoding="utf-8") as f:
        f.write(report)
    print(f"  â†’ {OUTPUT_REPORT} ã‚’å‡ºåŠ›")

    # å®Œäº†
    print("\n" + "=" * 60)
    print("Phase 2 å®Œäº†!")
    print("=" * 60)
    print("\nç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"  - {OUTPUT_JSONL}: è©•ä¾¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ç”¨")
    print(f"  - {generation_jsonl}: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«å­¦ç¿’ç”¨")
    print(f"  - {OUTPUT_REPORT}: ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ")
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: Phase 3 - ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ (Google Colabã§å®Ÿè¡Œ)")


if __name__ == "__main__":
    main()
    main()
