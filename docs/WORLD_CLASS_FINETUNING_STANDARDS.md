# 世界最高水準 LLM ファインチューニングスタンダード
>
> noteAI プロジェクト用リファレンスガイド

**作成日**: 2025年1月
**目的**: ネット上の複数ソースから収集した、世界レベルのファインチューニングベストプラクティス

---

## 📚 情報ソース一覧

| ソース | カテゴリ | URL/参照 |
|--------|----------|----------|
| Unsloth公式ドキュメント | LoRA/QLoRAハイパラ | unsloth.ai/docs |
| arXiv 2412.13337 | 学習率・スケジューラ研究 | arxiv.org/html/2412.13337v1 |
| Thinking Machines Blog | LoRA rank実験 | thinkingmachines.ai/blog/lora |
| JMIR AI Academic Paper | サンプルサイズ研究 | ai.jmir.org/2024/1/e52095 |
| Deepchecks | SOTA評価メトリクス | deepchecks.com |
| AIMultiple | LLM評価10+メトリクス | research.aimultiple.com |
| arXiv 2401.10359 | 過学習検出・防止 | arxiv.org/html/2401.10359v1 |
| V7 Labs | Train/Val/Test分割 | v7labs.com |
| SuperAnnotate | LLMファインチューニング2025 | superannotate.com |
| DextraLabs | PEFT技術ガイド | dextralabs.com |

---

## 1️⃣ データ品質基準

### 定義

「高品質データ」とは、タスクに直接関連し、ノイズがなく、一貫したフォーマットで、適切な多様性を持つデータセットのこと。

### 要点（複数ソースの合意）

| 基準 | 説明 | ソース |
|------|------|--------|
| **タスク関連性** | すべてのサンプルがターゲットタスクに直接対応 | DextraLabs, SuperAnnotate |
| **フォーマット一貫性** | 入力-出力ペアが統一されたフォーマット | 0xsojalsec Medium |
| **ノイズ除去** | タグリスト、不完全データ、ゴミデータの排除 | TheAlliance.ai |
| **エンティティ密度** | 実世界ケースに近い密度でデータを構成 | JMIR Academic Paper |
| **重複排除** | 同一または類似サンプルの除去 | 複数ソース共通 |

### noteAI v1の問題点との対比

```
❌ v1の問題:
- 28%がタグフォーマット汚染（「自己紹介｜子育て｜ワーママ」形式）
- 重複データが存在
- フォーマット不統一

✅ 世界基準:
- 100%がターゲットタスク（タイトル生成）に適合
- 重複なし
- 統一フォーマット
```

### 具体例

```json
// ❌ 低品質（noteAI v1で混入していたタイプ）
{"output": "自己紹介｜子育て｜ワーママ｜30代｜40代｜50代"}

// ✅ 高品質（世界基準に適合）
{"output": "【就職倍率100倍の公務員を辞めた】安定を捨てた20代の挫折と好転"}
{"output": "忙しい朝でも5分で完成！時短メイクの革命テクニック"}
{"output": "年収1000万を捨てて田舎移住した話"}
```

---

## 2️⃣ データセットサイズ基準

### 定義

適切なサンプル数とは、モデルがタスクを学習するのに十分だが、過学習を起こさない量のこと。

### 要点（学術研究ベース）

| 基準 | 推奨値 | ソース |
|------|--------|--------|
| **最小サンプル数** | 数百〜数千サンプル | 複数ソース共通 |
| **閾値効果** | 439-527文以上で精度向上が頭打ち | JMIR Academic (NER実験) |
| **品質 > 量** | 小規模でも高品質なら結果良好 | DextraLabs |
| **タスク複雑度依存** | 複雑なタスクほど多くのデータが必要 | 複数ソース共通 |

### 重要な発見

> "Sample sizes of greater than 439-527 sentences failed to produce meaningful accuracy improvements"
>
> — JMIR AI Academic Paper, 2024

**解釈**:

- 小規模データセットでも適切に構成すれば有効
- 品質とエンティティ密度が量より重要
- ただしこれはNERタスクの研究であり、生成タスクには直接適用不可

### noteAI v2への適用

| 項目 | v1（失敗） | v2（推奨） |
|------|------------|------------|
| サンプル数 | 638 | 500-1000（クリーン後） |
| 品質 | 28%汚染 | 100%クリーン |
| 多様性 | 不明 | 意図的に確保 |

---

## 3️⃣ Train/Validation/Test分割基準

### 定義

データを学習用・検証用・テスト用に分割し、過学習検出と汎化性能評価を可能にする手法。

### 要点（業界標準）

| 分割比率 | 用途 | ソース |
|----------|------|--------|
| **70/15/15** | 標準的な分割 | Milvus.io |
| **80/10/10** | 小規模データセット用 | V7 Labs |
| **80/20** | 最小構成（val/testを兼用） | Vicki Boykis |

### 分割時の注意点

| 注意点 | 説明 | ソース |
|--------|------|--------|
| **Stratified Sampling** | カテゴリ比率を維持して分割 | V7 Labs |
| **データリーク防止** | val/testの情報がtrainに混入しないよう | Lightly.ai |
| **グループ分割** | 関連データはまとめて同じセットに | Lightly.ai |
| **Augmentationの順序** | 分割後にAugmentation | Lightly.ai |

### noteAI v1の問題点

```
❌ v1の問題:
- Validation分割なし
- 過学習検出不能
- 汎化性能不明

✅ 世界基準:
- 最低70/15/15分割
- Validation lossの継続モニタリング
- Test setでの最終評価
```

---

## 4️⃣ LoRA/QLoRA ハイパーパラメータ基準

### 定義

Low-Rank Adaptation (LoRA) は、巨大モデルの一部のみを効率的に学習する手法。QLoRAは4bit量子化版。

### 要点（Unsloth公式 + 学術研究）

| パラメータ | 推奨値 | 理由 | ソース |
|------------|--------|------|--------|
| **lora_rank** | 16-64 | rank > 64は効果薄れる | Unsloth, ThinkingMachines |
| **lora_alpha** | rank × 1 または rank × 2 | alpha/rank = 1 or 2 が最適 | Unsloth |
| **rsLoRA** | True推奨 | sqrt(r)スケーリングで安定性向上 | rsLoRA論文 |
| **dropout** | 0.0-0.1 | 過学習リスク時は0.05-0.1 | 複数ソース |
| **target_modules** | 全attention層 | q,k,v,o,gate,up,down | Unsloth |

### Learning Rate基準

| 用途 | 推奨LR | ソース |
|------|--------|--------|
| LoRA/QLoRA SFT | 2e-4 (starting point) | Unsloth公式 |
| RL (DPO/GRPO) | 5e-6 | Unsloth公式 |
| Full Fine-tuning | より低いLR | arXiv研究 |

### 重要な発見

> "Our experiments showed that the optimal LR for LoRA is consistently 10x the one used for FullFT"
>
> — Thinking Machines Blog, 2025

### noteAI v1との比較

| パラメータ | v1設定 | 世界基準 | 評価 |
|------------|--------|----------|------|
| rank | 32 | 16-64 | ✅ 適正範囲 |
| alpha | 64 | rank×2=64 | ✅ 適正 |
| rsLoRA | True | True推奨 | ✅ 適正 |
| LR | 2e-4 | 2e-4 | ✅ 適正 |

**結論**: LoRA設定自体は問題なし。**問題はデータ品質**。

---

## 5️⃣ エポック数・過学習防止基準

### 定義

エポック数は全データを何回学習するか。過学習は訓練データを記憶して汎化できなくなる現象。

### 要点

| 基準 | 推奨 | ソース |
|------|------|--------|
| **SFT エポック数** | 1-3 epochs | Unsloth公式 |
| **過学習リスク** | 3エポック超えで急増 | Reddit, Unsloth |
| **Early Stopping** | patience 10-20 epochs | arXiv 2401.10359 |
| **Validation監視** | 必須 | 全ソース共通 |

### 過学習検出方法（学術研究）

| 方法 | 説明 | ソース |
|------|------|--------|
| **相関係数法** | train/val lossの相関が弱→過学習 | arXiv 2401.10359 |
| **Early Stopping** | val loss改善なしでpatience超過→停止 | TensorFlow/PyTorch標準 |
| **Smoothed Curves** | 移動平均でノイズ除去してから判定 | arXiv 2401.10359 |

### 過学習の危険信号

```
🚨 危険信号:
- Train Loss が極端に低い（< 0.3）のにVal Lossが高い
- 出力がデータセットの特定パターンを繰り返す
- 新規入力に対してゴミ出力

📊 noteAI v1の症状:
- Loss: 2.847 → 0.254 (91%減少)
- 3 epochs, 638サンプル
- 結果: 「」」」」」」」」」 繰り返し ← 典型的な過学習症状
```

### Reddit/GitHubでの警告

> "50 epochs is 100% overfitting. 10x your data and train for Max 5 epochs"
>
> — Reddit r/LocalLLaMA

---

## 6️⃣ 学習率スケジューラ基準

### 定義

学習率を訓練中に動的に変化させ、収束を安定させる手法。

### 要点

| スケジューラ | 説明 | 推奨度 | ソース |
|--------------|------|--------|--------|
| **Cosine Annealing** | 滑らかに減少→収束安定 | ⭐⭐⭐ | arXiv 2412.13337 |
| **Constant** | 一定（シンプル） | ⭐⭐ | ThinkingMachines |
| **Linear Decay** | 線形減少 | ⭐⭐ | 一般的 |

### Warmup Steps

| 基準 | 推奨 | ソース |
|------|------|--------|
| **Warmup比率** | 総ステップの3-10% | arXiv研究 |
| **目的** | 訓練初期の安定化 | arXiv 2412.13337 |
| **省略可能性** | 短い訓練では省略可能な場合も | arXiv 2412.13337 |

---

## 7️⃣ 推論検証基準（GGUF変換前）

### 定義

ファインチューニング後、GGUF変換前に行うべき品質検証プロセス。

### 要点（世界基準のワークフロー）

```
[訓練完了]
    ↓
[Step 1] HuggingFace形式で推論テスト
    ↓
[Step 2] 複数サンプルで出力確認
    ↓
[Step 3] F16 GGUFに変換
    ↓
[Step 4] F16で推論テスト
    ↓
[Step 5] 量子化（Q4_K_M等）
    ↓
[Step 6] 量子化版で推論テスト
    ↓
[Step 7] F16 vs 量子化の比較評価
    ↓
[デプロイ]
```

### 重要な検証ポイント

| 段階 | 確認項目 | 問題発生時の対応 |
|------|----------|------------------|
| **HF推論** | 意味のある出力が出るか | データ/訓練設定を見直し |
| **F16 GGUF** | HFと同等の出力か | 変換プロセスを確認 |
| **量子化版** | 品質劣化は許容範囲か | より高精度な量子化を試行 |

### noteAI v1の失敗

```
❌ noteAI v1の検証プロセス:
1. Colab上でLoRAマージ
2. Unslothでそのまま GGUF変換
3. いきなりQ4_K_Mで推論テスト → ゴミ出力
4. 「量子化のせいか？」と誤診断
5. F16テストで同様にゴミ → 実は訓練自体が失敗

✅ 世界基準では:
- HF形式での推論テストが最初
- 段階的な検証で問題箇所を特定
```

---

## 8️⃣ 評価メトリクス基準

### 定義

モデルの性能を定量的に測定する指標。

### タスク別推奨メトリクス（AIMultiple 2025）

| 評価対象 | ベンチマーク | 必須メトリクス |
|----------|--------------|----------------|
| コード生成 | HumanEval | Functional Correctness |
| 一般知識 | MMLU-Pro | Accuracy |
| ハルシネーション | TruthfulQA | Accuracy |
| 指示追従 | IFEval | Coherence |
| 言語理解 | BBH/SuperGLUE | Perplexity |
| 数学問題 | MATH | Accuracy |

### 自動評価 vs 人間評価（Deepchecks）

| 手法 | 長所 | 短所 | ソース |
|------|------|------|--------|
| **BLEU/ROUGE** | 高速、スケーラブル | 意味理解に弱い | Deepchecks |
| **BLEURT/NLI** | 意味的一貫性評価 | 長文に弱い | Deepchecks |
| **LLM-as-Judge** | 柔軟、人間的 | バイアスあり | Deepchecks |
| **人間評価** | 最も信頼性高 | コスト高、遅い | 全ソース |

### noteAIタスクへの適用

タイトル生成タスクには:

- **BLEU/ROUGE**: 参照タイトルとの類似度（限定的有用性）
- **人間評価**: クリック率予測、魅力度評価
- **LLM-as-Judge**: GPT-4等によるタイトル品質スコアリング

---

## 9️⃣ 完全チェックリスト（世界基準）

### Phase 1: データ準備

- [ ] **タスク定義の明確化**: 「note.com記事のタイトル生成」
- [ ] **データソース選定**: 高品質な記事-タイトルペア
- [ ] **データクリーニング**:
  - [ ] タグフォーマット除去
  - [ ] 重複排除
  - [ ] 不完全データ除去
  - [ ] フォーマット統一
- [ ] **品質検証**: 全サンプルを目視またはルールベースでチェック
- [ ] **Train/Val/Test分割**: 70/15/15 または 80/10/10
- [ ] **サンプル数確認**: 最低数百、理想は数千

### Phase 2: モデル・訓練設定

- [ ] **ベースモデル選定**: タスクに適したモデル
- [ ] **LoRA設定**:
  - [ ] rank: 16-64
  - [ ] alpha: rank × 1 or 2
  - [ ] rsLoRA: True（推奨）
- [ ] **Learning Rate**: 2e-4 (LoRA/QLoRA SFT)
- [ ] **エポック数**: 1-3
- [ ] **バッチサイズ**: VRAM許容範囲で最大化
- [ ] **スケジューラ**: Cosine Annealing推奨

### Phase 3: 訓練・モニタリング

- [ ] **Validation Loss追跡**: 各ステップでログ
- [ ] **Train vs Val Loss比較**: 乖離→過学習
- [ ] **Early Stopping設定**: patience 10-20
- [ ] **チェックポイント保存**: 定期的に保存

### Phase 4: 推論検証

- [ ] **HuggingFace推論テスト**: GGUF変換前に必須
- [ ] **複数プロンプトでテスト**: 多様な入力で確認
- [ ] **出力品質評価**: 意味のある出力か
- [ ] **F16 GGUF変換**: 最初は高精度で
- [ ] **F16推論テスト**: HFと同等か確認
- [ ] **量子化**: Q4_K_M等
- [ ] **量子化版テスト**: 品質劣化は許容範囲か

### Phase 5: 最終評価・デプロイ

- [ ] **Test setでの評価**: 未使用データで最終評価
- [ ] **定量メトリクス**: BLEU/ROUGE/Perplexity等
- [ ] **定性評価**: 人間またはLLM-as-Judge
- [ ] **A/Bテスト**: 可能なら実運用環境で

---

## 🔟 noteAI v2 推奨設定

上記の世界基準を踏まえたnoteAI v2の推奨設定:

### データ

| 項目 | v1（失敗） | v2（推奨） |
|------|------------|------------|
| サンプル数 | 638 | 500-1000（クリーン） |
| 汚染率 | 28% | 0% |
| Train/Val分割 | なし | 80/20 |
| 重複 | あり | なし |

### 訓練設定

| 項目 | v1 | v2（推奨） |
|------|-----|------------|
| rank | 32 | 32 (OK) |
| alpha | 64 | 64 (OK) |
| LR | 2e-4 | 2e-4 (OK) |
| エポック | 3 | 1-2 (削減) |
| Validation | なし | **あり（必須）** |
| Early Stopping | なし | **あり（必須）** |

### 検証プロセス

```
[クリーンデータ準備]
    ↓
[80/20分割]
    ↓
[訓練開始 + Val Loss監視]
    ↓
[Early Stopping発動 or 1-2エポック完了]
    ↓
[HF形式で推論テスト] ← ここで品質確認！
    ↓
[問題なければF16 GGUF変換]
    ↓
[F16推論テスト]
    ↓
[問題なければQ4_K_M変換]
    ↓
[最終テスト]
```

---

## 📝 まとめ

### 失敗の根本原因（再確認）

| カテゴリ | 問題 | 世界基準との乖離 |
|----------|------|------------------|
| **データ品質** | 28%汚染 | 0%であるべき |
| **検証プロセス** | Val分割なし | 必須 |
| **推論テスト** | GGUF後に初めて | HF段階でテストすべき |
| **過学習検出** | なし | Loss監視必須 |

### 世界基準の核心

1. **Quality > Quantity**: 小規模でも高品質データが勝つ
2. **Always Validate**: 検証なしの訓練は盲目飛行
3. **Test Before Convert**: GGUF変換前にHFで必ずテスト
4. **Monitor Everything**: Train/Val Lossの継続監視
5. **1-3 Epochs**: 過学習リスクを最小化

---

*このドキュメントは2025年1月時点のWeb調査に基づく。技術は急速に進化するため、定期的な更新を推奨。*
