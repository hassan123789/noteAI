"""
noteAI - 2026 World-Class Inference Script
==========================================

Unsloth + Qwen3-8B ã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸæ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ¨å¥¨ãƒ¢ãƒ‡ãƒ«:
    - Qwen3-8B: 16GB VRAMå‘ã‘ï¼ˆæ—¥æœ¬èªæœ€å¼·ï¼‰
    - Qwen3-4B: 8GB VRAMå‘ã‘ï¼ˆ120BåŒ¹æ•µã®æ€§èƒ½ï¼‰

ä½¿ç”¨æ–¹æ³•:
    python inference_2026.py --model path/to/model --category "å‰¯æ¥­"
    python inference_2026.py --model path/to/model --interactive
"""

import argparse
import json
from pathlib import Path

import torch


def load_model_unsloth(model_path: str):
    """Unslothã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    from unsloth import FastLanguageModel

    print(f"Loading model: {model_path}")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_path,
        max_seq_length=512,
        dtype=None,
        load_in_4bit=True,
    )
    FastLanguageModel.for_inference(model)

    return model, tokenizer


def load_model_transformers(model_path: str):
    """Transformersã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆUnslothãªã—ï¼‰"""
    from transformers import AutoModelForCausalLM, AutoTokenizer

    print(f"Loading model: {model_path}")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto",
    )

    return model, tokenizer


def generate_title(
    model,
    tokenizer,
    category: str,
    temperature: float = 0.7,
    max_new_tokens: int = 64,
) -> str:
    """ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ"""

    # ChatMLå½¢å¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    messages = [
        {
            "role": "system",
            "content": "ã‚ãªãŸã¯note.comã®äººæ°—è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹å°‚é–€AIã§ã™ã€‚èª­è€…ã®èˆˆå‘³ã‚’å¼•ãã€ã‚¯ãƒªãƒƒã‚¯ã—ãŸããªã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚"
        },
        {
            "role": "user",
            "content": f"ã€Œ{category}ã€ã«é–¢ã™ã‚‹ã€èª­è€…ã‚’æƒ¹ãã¤ã‘ã‚‹noteè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’1ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
        },
    ]

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
    ).to(model.device)

    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id,
        )

    # ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆç”Ÿæˆéƒ¨åˆ†ã®ã¿ï¼‰
    generated = tokenizer.decode(
        outputs[0][inputs.shape[1]:],
        skip_special_tokens=True
    )

    # æœ€åˆã®è¡Œã®ã¿
    title = generated.strip().split("\n")[0]

    return title


def generate_multiple_titles(
    model,
    tokenizer,
    category: str,
    n: int = 5,
) -> list:
    """è¤‡æ•°ã®ã‚¿ã‚¤ãƒˆãƒ«ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
    titles = []
    for i in range(n):
        temp = 0.6 + (i * 0.1)  # 0.6, 0.7, 0.8, 0.9, 1.0
        title = generate_title(model, tokenizer, category, temperature=temp)
        titles.append(title)
    return titles


def interactive_mode(model, tokenizer):
    """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
    print("\n" + "="*60)
    print("ğŸ¯ noteAI Interactive Mode")
    print("="*60)
    print("ã‚«ãƒ†ã‚´ãƒªã‚’å…¥åŠ›ã—ã¦ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
    print("'quit' ã¾ãŸã¯ 'exit' ã§çµ‚äº†ã€‚")
    print("'multi' + ã‚«ãƒ†ã‚´ãƒª ã§5ã¤ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã€‚")
    print("="*60)

    while True:
        try:
            user_input = input("\nğŸ“ ã‚«ãƒ†ã‚´ãƒª: ").strip()

            if not user_input:
                continue

            if user_input.lower() in ["quit", "exit", "q"]:
                print("ğŸ‘‹ çµ‚äº†ã—ã¾ã™ã€‚")
                break

            # è¤‡æ•°ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰
            if user_input.lower().startswith("multi "):
                category = user_input[6:].strip()
                print(f"\nã€{category}ã€‘ã®ã‚¿ã‚¤ãƒˆãƒ«å€™è£œ:")
                print("-" * 40)
                titles = generate_multiple_titles(model, tokenizer, category)
                for i, title in enumerate(titles, 1):
                    print(f"  {i}. {title}")
            else:
                # å˜ä¸€ç”Ÿæˆ
                title = generate_title(model, tokenizer, user_input)
                print(f"\nâœ¨ ç”Ÿæˆã‚¿ã‚¤ãƒˆãƒ«: {title}")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ çµ‚äº†ã—ã¾ã™ã€‚")
            break


def main():
    parser = argparse.ArgumentParser(
        description="noteAI Title Generator - 2026 World-Class"
    )
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ï¼ˆLoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¾ãŸã¯ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼‰"
    )
    parser.add_argument(
        "--category",
        type=str,
        default=None,
        help="ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã‚«ãƒ†ã‚´ãƒª"
    )
    parser.add_argument(
        "--interactive",
        action="store_true",
        help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•"
    )
    parser.add_argument(
        "--n",
        type=int,
        default=1,
        help="ç”Ÿæˆã™ã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã®æ•°"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="ç”Ÿæˆæ¸©åº¦ï¼ˆ0.1-1.5ï¼‰"
    )
    parser.add_argument(
        "--use-transformers",
        action="store_true",
        help="Unslothã®ä»£ã‚ã‚Šã«Transformersã‚’ä½¿ç”¨"
    )

    args = parser.parse_args()

    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    if args.use_transformers:
        model, tokenizer = load_model_transformers(args.model)
    else:
        try:
            model, tokenizer = load_model_unsloth(args.model)
        except ImportError:
            print("âš ï¸ UnslothãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Transformersã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            model, tokenizer = load_model_transformers(args.model)

    print("âœ… Model loaded!")

    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
    if args.interactive:
        interactive_mode(model, tokenizer)
    elif args.category:
        if args.n > 1:
            print(f"\nã€{args.category}ã€‘ã®ã‚¿ã‚¤ãƒˆãƒ«å€™è£œ:")
            print("-" * 40)
            titles = generate_multiple_titles(model, tokenizer, args.category, n=args.n)
            for i, title in enumerate(titles, 1):
                print(f"  {i}. {title}")
        else:
            title = generate_title(
                model, tokenizer, args.category,
                temperature=args.temperature
            )
            print(f"\nâœ¨ ç”Ÿæˆã‚¿ã‚¤ãƒˆãƒ«: {title}")
    else:
        print("ã‚¨ãƒ©ãƒ¼: --category ã¾ãŸã¯ --interactive ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        parser.print_help()


if __name__ == "__main__":
    main()
