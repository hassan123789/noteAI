{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a770dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "!pip install -q transformers peft accelerate bitsandbytes datasets trl\n",
    "!pip install -q sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c53cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveãƒã‚¦ãƒ³ãƒˆï¼ˆãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ï¼‰\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹è¨­å®š\n",
    "DATA_PATH = Path(\"/content/drive/MyDrive/noteAI/data\")\n",
    "OUTPUT_PATH = Path(\"/content/drive/MyDrive/noteAI/models\")\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d1595",
   "metadata": {},
   "source": [
    "## è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9ec149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š ===\n",
    "CONFIG = {\n",
    "    # ãƒ¢ãƒ‡ãƒ«\n",
    "    \"base_model\": \"rinna/japanese-gpt-neox-3.6b\",\n",
    "\n",
    "    # LoRAè¨­å®šï¼ˆ2026å¹´ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ï¼‰\n",
    "    \"lora_r\": 64,           # rsLoRAã§ã¯é«˜rankæ¨å¥¨ï¼ˆ8â†’64ï¼‰\n",
    "    \"lora_alpha\": 128,      # é€šå¸¸ 2*r\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"use_rslora\": True,     # â˜… rsLoRAæœ‰åŠ¹åŒ–\n",
    "    \"use_dora\": True,       # â˜… DoRAæœ‰åŠ¹åŒ–ï¼ˆ2024 ICMLï¼‰\n",
    "\n",
    "    # QLoRAè¨­å®š\n",
    "    \"use_4bit\": True,\n",
    "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "\n",
    "    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "    \"max_seq_length\": 256,\n",
    "    \"batch_size\": 4,\n",
    "    \"gradient_accumulation\": 4,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"weight_decay\": 0.001,\n",
    "\n",
    "    # å‡ºåŠ›\n",
    "    \"output_name\": \"noteai-title-generator-v3\",\n",
    "}\n",
    "\n",
    "print(\"è¨­å®š:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70d64cb",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data():\n",
    "    \"\"\"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿\"\"\"\n",
    "    data = []\n",
    "\n",
    "    # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿\n",
    "    main_file = DATA_PATH / \"augmented\" / \"augmented_training.jsonl\"\n",
    "    if main_file.exists():\n",
    "        with open(main_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    entry = json.loads(line)\n",
    "                    data.append(entry)\n",
    "                except:\n",
    "                    pass\n",
    "        print(f\"ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿: {len(data)}ä»¶\")\n",
    "\n",
    "    # Evol-Instructãƒ‡ãƒ¼ã‚¿\n",
    "    evol_file = DATA_PATH / \"processed\" / \"evol_instruct_data.jsonl\"\n",
    "    evol_count = 0\n",
    "    if evol_file.exists():\n",
    "        with open(evol_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    entry = json.loads(line)\n",
    "                    data.append(entry)\n",
    "                    evol_count += 1\n",
    "                except:\n",
    "                    pass\n",
    "        print(f\"Evol-Instructãƒ‡ãƒ¼ã‚¿: {evol_count}ä»¶\")\n",
    "\n",
    "    print(f\"åˆè¨ˆ: {len(data)}ä»¶\")\n",
    "    return data\n",
    "\n",
    "raw_data = load_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_training_example(entry):\n",
    "    \"\"\"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›\"\"\"\n",
    "\n",
    "    # Evol-Instructå½¢å¼\n",
    "    if \"instruction\" in entry:\n",
    "        instruction = entry[\"instruction\"]\n",
    "        input_text = entry.get(\"input\", \"\")\n",
    "        output = entry.get(\"output\", \"\")\n",
    "\n",
    "        if input_text:\n",
    "            prompt = f\"### æŒ‡ç¤º:\\n{instruction}\\n\\n### å…¥åŠ›:\\n{input_text}\\n\\n### å‡ºåŠ›:\\n{output}\"\n",
    "        else:\n",
    "            prompt = f\"### æŒ‡ç¤º:\\n{instruction}\\n\\n### å‡ºåŠ›:\\n{output}\"\n",
    "\n",
    "    # é€šå¸¸å½¢å¼ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ï¼‰\n",
    "    else:\n",
    "        title = entry.get(\"title\", \"\")\n",
    "        category = entry.get(\"category\", \"\")\n",
    "\n",
    "        prompt = f\"### æŒ‡ç¤º:\\nä»¥ä¸‹ã®ã‚«ãƒ†ã‚´ãƒªã«é–¢ã™ã‚‹ã€èª­è€…ã‚’æƒ¹ãã¤ã‘ã‚‹noteè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\\n\\n### ã‚«ãƒ†ã‚´ãƒª:\\n{category}\\n\\n### ã‚¿ã‚¤ãƒˆãƒ«:\\n{title}\"\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
    "formatted_data = [format_training_example(e) for e in raw_data]\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "print(f\"\\nãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(dataset)}ä»¶\")\n",
    "print(f\"\\nä¾‹:\")\n",
    "print(dataset[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a30ebf7",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆQLoRAï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08983d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bité‡å­åŒ–è¨­å®š\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=CONFIG[\"use_4bit\"],\n",
    "    bnb_4bit_quant_type=CONFIG[\"bnb_4bit_quant_type\"],\n",
    "    bnb_4bit_compute_dtype=getattr(torch, CONFIG[\"bnb_4bit_compute_dtype\"]),\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
    "print(f\"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­: {CONFIG['base_model']}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"base_model\"],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"base_model\"],\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9e3f2e",
   "metadata": {},
   "source": [
    "## rsLoRA + DoRA è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae433602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kbit trainingæº–å‚™\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRAè¨­å®šï¼ˆrsLoRA + DoRAï¼‰\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "\n",
    "    # â˜… 2026å¹´ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n",
    "    use_rslora=CONFIG[\"use_rslora\"],     # rsLoRA: âˆšr ã§é™¤ç®—\n",
    "    use_dora=CONFIG[\"use_dora\"],         # DoRA: æ–¹å‘ã¨å¤§ãã•ã‚’åˆ†é›¢\n",
    "\n",
    "    # å…¨å±¤ã«LoRAã‚’é©ç”¨ï¼ˆSebastian Raschkaæ¨å¥¨ï¼‰\n",
    "    target_modules=[\n",
    "        \"query_key_value\",\n",
    "        \"dense\",\n",
    "        \"dense_h_to_4h\",\n",
    "        \"dense_4h_to_h\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# PEFTãƒ¢ãƒ‡ãƒ«ä½œæˆ\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\nğŸ“Š LoRAè¨­å®š:\")\n",
    "print(f\"  rank: {CONFIG['lora_r']}\")\n",
    "print(f\"  alpha: {CONFIG['lora_alpha']}\")\n",
    "print(f\"  rsLoRA: {CONFIG['use_rslora']}\")\n",
    "print(f\"  DoRA: {CONFIG['use_dora']}\")\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20344408",
   "metadata": {},
   "source": [
    "## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6116c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_PATH / CONFIG[\"output_name\"]),\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "\n",
    "    # æœ€é©åŒ–\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    # ãƒ­ã‚°\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    # ãã®ä»–\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"âœ… ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼æº–å‚™å®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de20fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\n",
    "print(\"ğŸš€ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...\")\n",
    "trainer.train()\n",
    "print(\"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5499ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "save_path = OUTPUT_PATH / CONFIG[\"output_name\"] / \"final\"\n",
    "trainer.save_model(str(save_path))\n",
    "tokenizer.save_pretrained(str(save_path))\n",
    "\n",
    "print(f\"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e0d55",
   "metadata": {},
   "source": [
    "## æ¨è«–ãƒ†ã‚¹ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_title(category: str, model, tokenizer, max_new_tokens: int = 64):\n",
    "    \"\"\"ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ\"\"\"\n",
    "    prompt = f\"### æŒ‡ç¤º:\\nä»¥ä¸‹ã®ã‚«ãƒ†ã‚´ãƒªã«é–¢ã™ã‚‹ã€èª­è€…ã‚’æƒ¹ãã¤ã‘ã‚‹noteè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\\n\\n### ã‚«ãƒ†ã‚´ãƒª:\\n{category}\\n\\n### ã‚¿ã‚¤ãƒˆãƒ«:\\n\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    title = generated.split(\"### ã‚¿ã‚¤ãƒˆãƒ«:\\n\")[-1].strip()\n",
    "\n",
    "    # æœ€åˆã®è¡Œã®ã¿\n",
    "    title = title.split(\"\\n\")[0].strip()\n",
    "\n",
    "    return title\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆ\n",
    "test_categories = [\"å‰¯æ¥­\", \"è‹±èªå­¦ç¿’\", \"ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°\", \"æŠ•è³‡\", \"ãƒ–ãƒ­ã‚°\"]\n",
    "\n",
    "print(\"\\nğŸ¯ ç”Ÿæˆãƒ†ã‚¹ãƒˆ:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for cat in test_categories:\n",
    "    title = generate_title(cat, model, tokenizer)\n",
    "    print(f\"[{cat}] {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7738b2ba",
   "metadata": {},
   "source": [
    "## å®Œäº†\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã¯ Google Drive ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ:\n",
    "- `/content/drive/MyDrive/noteAI/models/noteai-title-generator-v3/final/`\n",
    "\n",
    "ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦æ¨è«–ã™ã‚‹ã«ã¯ `inference.py` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
