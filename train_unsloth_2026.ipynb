{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ã‚»ãƒ«1: Unsloth ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "# ============================================================\n",
    "%%capture\n",
    "!pip install unsloth\n",
    "# æœ€æ–°ç‰ˆã‚’å–å¾—ï¼ˆColabç”¨ï¼‰\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cbbdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ã‚»ãƒ«2: ã‚¤ãƒ³ãƒãƒ¼ãƒˆ & GPUç¢ºèª\n",
    "# ============================================================\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ noteAI 2026 World-Class Training\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name} ({gpu_mem:.1f}GB)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf580973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ã‚»ãƒ«3: è¨­å®šï¼ˆ2026å¹´ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ï¼‰\n",
    "# ============================================================\n",
    "\n",
    "# === ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¬ã‚¤ãƒ‰ï¼ˆVRAMåˆ¥æ¨å¥¨ - 2025-2026èª¿æŸ»çµæœï¼‰ ===\n",
    "#\n",
    "# ã€ã‚ãªãŸã®RTX 5060 Ti 16GB VRAM â†’ Qwen3-14B ãŒå¯èƒ½ï¼ã€‘\n",
    "#\n",
    "# | VRAM     | æ¨å¥¨ãƒ¢ãƒ‡ãƒ«            | 4bit VRAM | å­¦ç¿’æ™‚VRAM | å‚™è€ƒ              |\n",
    "# |----------|----------------------|-----------|------------|-------------------|\n",
    "# | ~8GB     | Qwen3-4B-bnb-4bit    | ~4GB      | ~6-8GB     | 120BåŒ¹æ•µã®æ€§èƒ½    |\n",
    "# | ~12GB    | Qwen3-14B-bnb-4bit   | ~10GB     | ~12GB      | â˜… 12GBå¯èƒ½ï¼ˆå®Ÿè¨¼æ¸ˆï¼‰|\n",
    "# | ~16GB+   | Qwen3-14B-bnb-4bit   | ~10GB     | ~12-14GB   | â˜…â˜… æœ€é©ï¼        |\n",
    "# | ~17.5GB+ | Qwen3-30B-A3B (MoE)  | ~15GB     | ~17.5GB    | æœ€å¼·ã‚¯ãƒ©ã‚¹        |\n",
    "#\n",
    "# èª¿æŸ»çµæœï¼ˆ2025-2026 Webæ¤œç´¢ï¼‰:\n",
    "# - YouTubeå®Ÿè¨¼: ã€ŒQwen3-14Bã‚’12GB VRAMã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ã€\n",
    "# - Unslothå…¬å¼: ã€ŒQwen3-30B-A3BãŒ17.5GB VRAMã§å‹•ä½œã€\n",
    "# - 4bité‡å­åŒ–: ç²¾åº¦ä½ä¸‹2-5%ã€Unslothæœ€é©åŒ–ã§ç²¾åº¦ä½ä¸‹ã‚¼ãƒ­é”æˆ\n",
    "# - rsLoRA + å…¨å±¤ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã§æœ€é«˜ç²¾åº¦é”æˆ\n",
    "#\n",
    "# é‡å­åŒ–ã®ä»•çµ„ã¿:\n",
    "# - bnb = bitsandbytesï¼ˆé‡å­åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰\n",
    "# - 4bit = 4bité‡å­åŒ–ï¼ˆVRAM 75%å‰Šæ¸›ã€é€Ÿåº¦2.4xå‘ä¸Šï¼‰\n",
    "# - nf4 = Normal Float 4ï¼ˆæƒ…å ±å¯†åº¦æœ€é©åŒ–ï¼‰\n",
    "\n",
    "CONFIG = {\n",
    "    # === ãƒ¢ãƒ‡ãƒ« ===\n",
    "    # â˜…â˜… Qwen3-14B: RTX 5060 Ti 16GB VRAMã§æœ€å¼·ãƒ¢ãƒ‡ãƒ«ï¼\n",
    "    # 4bité‡å­åŒ–ã§ç´„10GBã€å­¦ç¿’æ™‚12-14GBä½¿ç”¨ï¼ˆYouTubeå®Ÿè¨¼æ¸ˆã¿ï¼‰\n",
    "    \"model_name\": \"unsloth/Qwen3-14B-bnb-4bit\",\n",
    "\n",
    "    # === ä»£æ›¿ãƒ¢ãƒ‡ãƒ« ===\n",
    "    # \"model_name\": \"unsloth/Qwen3-8B-bnb-4bit\",   # å®‰å…¨ç­–ï¼ˆ~10-12GBä½¿ç”¨ï¼‰\n",
    "    # \"model_name\": \"unsloth/Qwen3-4B-bnb-4bit\",   # 8GBä»¥ä¸‹ã®VRAMç”¨\n",
    "\n",
    "    # === LoRAè¨­å®šï¼ˆèª¿æŸ»çµæœã«åŸºã¥ãæœ€é©å€¤ï¼‰ ===\n",
    "    \"lora_r\": 32,              # rank: 16-32ã§ååˆ†ï¼ˆèª¿æŸ»çµæœï¼‰\n",
    "    \"lora_alpha\": 64,          # é€šå¸¸ 2*rï¼ˆrsLoRAã§è‡ªå‹•èª¿æ•´ï¼‰\n",
    "    \"lora_dropout\": 0.0,       # Unslothã§ã¯0æ¨å¥¨ï¼ˆæœ€é©åŒ–æ¸ˆã¿ï¼‰\n",
    "    \"use_rslora\": True,        # â˜… rsLoRA: é«˜rankå®‰å®šåŒ–ï¼ˆÎ±/âˆšr scalingï¼‰\n",
    "    # DoRAã¯ç„¡åŠ¹ï¼ˆ8xã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ - èª¿æŸ»çµæœï¼‰\n",
    "\n",
    "    # === ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆå…¨å±¤ - èª¿æŸ»çµæœã§æœ€é«˜ç²¾åº¦ï¼‰ ===\n",
    "    # QLoRA-All: attention + MLP å…¨å±¤ãŒæœ€é«˜ã‚¹ã‚³ã‚¢\n",
    "    \"target_modules\": [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",     # MLP/FFN\n",
    "    ],\n",
    "\n",
    "    # === ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ===\n",
    "    \"max_seq_length\": 512,     # ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆãªã®ã§çŸ­ã‚ã§OK\n",
    "    \"batch_size\": 2,           # â˜… 14Bç”¨ã«èª¿æ•´ï¼ˆVRAMç¯€ç´„ï¼‰\n",
    "    \"gradient_accumulation\": 8, # å®ŸåŠ¹batch=16ã‚’ç¶­æŒ\n",
    "    \"learning_rate\": 2e-4,     # LoRAæ¨™æº–\n",
    "    \"num_epochs\": 3,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"weight_decay\": 0.01,\n",
    "\n",
    "    # === å‡ºåŠ› ===\n",
    "    \"output_name\": \"noteai-qwen3-14b-title-v1\",\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‹ Training Configuration (2026 Best Practices):\")\n",
    "print(\"-\" * 50)\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ã‚»ãƒ«4: Google Drive ãƒã‚¦ãƒ³ãƒˆ\n",
    "# ============================================================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ãƒ‘ã‚¹è¨­å®š\n",
    "DATA_PATH = Path(\"/content/drive/MyDrive/noteAI/data\")\n",
    "OUTPUT_PATH = Path(\"/content/drive/MyDrive/noteAI/models\")\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“ Data: {DATA_PATH}\")\n",
    "print(f\"ğŸ“ Output: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ã‚»ãƒ«5: Unsloth ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆ2-5xé«˜é€Ÿï¼‰\n",
    "# ============================================================\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(f\"ğŸ”„ Loading: {CONFIG['model_name']}\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
    "    dtype=None,              # è‡ªå‹•æ¤œå‡º\n",
    "    load_in_4bit=True,       # QLoRA\n",
    ")\n",
    "\n",
    "print(\"âœ… Model loaded with Unsloth optimization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a22a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ã‚»ãƒ«6: LoRAè¨­å®šï¼ˆrsLoRA + å…¨å±¤ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰\n",
    "# ============================================================\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "\n",
    "    # â˜… å…¨å±¤ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆèª¿æŸ»çµæœ: QLoRA-All ãŒæœ€é«˜ç²¾åº¦ï¼‰\n",
    "    target_modules=CONFIG[\"target_modules\"],\n",
    "\n",
    "    # â˜… rsLoRAæœ‰åŠ¹åŒ–ï¼ˆé«˜rankå®‰å®šåŒ–ï¼‰\n",
    "    use_rslora=CONFIG[\"use_rslora\"],\n",
    "\n",
    "    # è¿½åŠ è¨­å®š\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Unslothæœ€é©åŒ–\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“Š LoRA Configuration:\")\n",
    "print(f\"  Rank (r): {CONFIG['lora_r']}\")\n",
    "print(f\"  Alpha: {CONFIG['lora_alpha']}\")\n",
    "print(f\"  rsLoRA: {CONFIG['use_rslora']} (Î±/âˆšr scaling)\")\n",
    "print(f\"  Target: {len(CONFIG['target_modules'])} modules (attention + MLP)\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0771f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ã‚»ãƒ«7: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "# ============================================================\n",
    "\n",
    "def load_training_data():\n",
    "    \"\"\"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿\"\"\"\n",
    "    data = []\n",
    "\n",
    "    # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿\n",
    "    augmented_file = DATA_PATH / \"augmented\" / \"augmented_training.jsonl\"\n",
    "    if augmented_file.exists():\n",
    "        with open(augmented_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except:\n",
    "                    pass\n",
    "        print(f\"  âœ“ Augmented: {len(data)} samples\")\n",
    "\n",
    "    # Evol-Instructãƒ‡ãƒ¼ã‚¿\n",
    "    evol_file = DATA_PATH / \"processed\" / \"evol_instruct_data.jsonl\"\n",
    "    evol_count = 0\n",
    "    if evol_file.exists():\n",
    "        with open(evol_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                    evol_count += 1\n",
    "                except:\n",
    "                    pass\n",
    "        print(f\"  âœ“ Evol-Instruct: {evol_count} samples\")\n",
    "\n",
    "    # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿\n",
    "    processed_file = DATA_PATH / \"processed\" / \"training_data.jsonl\"\n",
    "    processed_count = 0\n",
    "    if processed_file.exists():\n",
    "        with open(processed_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                    processed_count += 1\n",
    "                except:\n",
    "                    pass\n",
    "        print(f\"  âœ“ Processed: {processed_count} samples\")\n",
    "\n",
    "    return data\n",
    "\n",
    "print(\"ğŸ“‚ Loading training data...\")\n",
    "raw_data = load_training_data()\n",
    "print(f\"\\nğŸ“Š Total: {len(raw_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1495d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ã‚»ãƒ«8: ChatMLå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆQwen3æœ€é©ï¼‰\n",
    "# ============================================================\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Qwen3ç”¨ChatMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"qwen3\",  # â˜… Qwen3ç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\n",
    ")\n",
    "\n",
    "def format_to_chatml(entry):\n",
    "    \"\"\"\n",
    "    ChatMLå½¢å¼ã«å¤‰æ›ï¼ˆInstructå‘ã‘æœ€é© - èª¿æŸ»çµæœï¼‰\n",
    "    \"\"\"\n",
    "    # Evol-Instructå½¢å¼\n",
    "    if \"instruction\" in entry:\n",
    "        instruction = entry[\"instruction\"]\n",
    "        input_text = entry.get(\"input\", \"\")\n",
    "        output = entry.get(\"output\", \"\")\n",
    "\n",
    "        if input_text:\n",
    "            user_content = f\"{instruction}\\n\\nå…¥åŠ›:\\n{input_text}\"\n",
    "        else:\n",
    "            user_content = instruction\n",
    "        assistant_content = output\n",
    "\n",
    "    # é€šå¸¸å½¢å¼ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ï¼‰\n",
    "    else:\n",
    "        title = entry.get(\"title\", \"\")\n",
    "        category = entry.get(\"category\", \"ä¸€èˆ¬\")\n",
    "\n",
    "        user_content = f\"ã€Œ{category}ã€ã«é–¢ã™ã‚‹ã€èª­è€…ã‚’æƒ¹ãã¤ã‘ã‚‹noteè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’1ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\"\n",
    "        assistant_content = title\n",
    "\n",
    "    # ChatMLå½¢å¼\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"ã‚ãªãŸã¯note.comã®äººæ°—è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹å°‚é–€AIã§ã™ã€‚èª­è€…ã®èˆˆå‘³ã‚’å¼•ãã€ã‚¯ãƒªãƒƒã‚¯ã—ãŸããªã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚\"},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "    ]\n",
    "\n",
    "    return {\"conversations\": messages}\n",
    "\n",
    "# å¤‰æ›\n",
    "formatted_data = [format_to_chatml(e) for e in raw_data if e]\n",
    "\n",
    "print(f\"âœ… Formatted: {len(formatted_data)} samples\")\n",
    "print(\"\\nğŸ“ Example:\")\n",
    "print(\"-\" * 50)\n",
    "for msg in formatted_data[0][\"conversations\"]:\n",
    "    print(f\"[{msg['role']}]: {msg['content'][:100]}...\" if len(msg['content']) > 100 else f\"[{msg['role']}]: {msg['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8989eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ã‚»ãƒ«9: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆCompletions Only Trainingï¼‰\n",
    "# ============================================================\n",
    "from unsloth.chat_templates import standardize_sharegpt, train_on_responses_only\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "def apply_chat_template(examples):\n",
    "    \"\"\"ChatMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨\"\"\"\n",
    "    texts = []\n",
    "    for convs in examples[\"conversations\"]:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            convs,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(apply_chat_template, batched=True)\n",
    "\n",
    "print(f\"âœ… Dataset ready: {len(dataset)} samples\")\n",
    "print(\"\\nğŸ“ Tokenized example:\")\n",
    "print(\"-\" * 50)\n",
    "print(dataset[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f04ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ã‚»ãƒ«10: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š\n",
    "# ============================================================\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_PATH / CONFIG[\"output_name\"]),\n",
    "\n",
    "    # ã‚¨ãƒãƒƒã‚¯ãƒ»ãƒãƒƒãƒ\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation\"],\n",
    "\n",
    "    # å­¦ç¿’ç‡ï¼ˆLoRAæ¨™æº–ï¼‰\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "\n",
    "    # æœ€é©åŒ–\n",
    "    optim=\"adamw_8bit\",        # Unslothæ¨å¥¨\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    # ç²¾åº¦\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "\n",
    "    # ãƒ­ã‚°\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    # ãã®ä»–\n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"âœ… Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d792c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ã‚»ãƒ«11: SFTTrainer + Completions Only Training\n",
    "# ============================================================\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,  # ChatMLã§ã¯packingã‚ªãƒ•æ¨å¥¨\n",
    ")\n",
    "\n",
    "# â˜… Completions Only Trainingï¼ˆèª¿æŸ»çµæœ: +1%ç²¾åº¦å‘ä¸Šï¼‰\n",
    "# ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å›ç­”éƒ¨åˆ†ã®ã¿ã§å­¦ç¿’\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|im_start|>user\\n\",\n",
    "    response_part=\"<|im_start|>assistant\\n\",\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer ready with Completions-Only Training\")\n",
    "print(\"   (Training only on assistant responses for better accuracy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c91db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ã‚»ãƒ«12: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\n",
    "# ============================================================\n",
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸš€ TRAINING START\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(f\"Samples: {len(dataset)}\")\n",
    "print(f\"Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"Effective batch: {CONFIG['batch_size'] * CONFIG['gradient_accumulation']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Time: {elapsed/60:.1f} minutes\")\n",
    "print(f\"Loss: {trainer_stats.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d2ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ã‚»ãƒ«13: ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆè¤‡æ•°å½¢å¼ï¼‰\n",
    "# ============================================================\n",
    "\n",
    "save_path = OUTPUT_PATH / CONFIG[\"output_name\"]\n",
    "\n",
    "# 1. LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ã¿ä¿å­˜\n",
    "print(\"ğŸ’¾ Saving LoRA adapter...\")\n",
    "model.save_pretrained(str(save_path / \"lora_adapter\"))\n",
    "tokenizer.save_pretrained(str(save_path / \"lora_adapter\"))\n",
    "print(f\"   â†’ {save_path / 'lora_adapter'}\")\n",
    "\n",
    "# 2. ãƒãƒ¼ã‚¸æ¸ˆã¿16bitï¼ˆæ¨è«–ç”¨ï¼‰\n",
    "print(\"\\nğŸ’¾ Saving merged 16bit model...\")\n",
    "model.save_pretrained_merged(\n",
    "    str(save_path / \"merged_16bit\"),\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "print(f\"   â†’ {save_path / 'merged_16bit'}\")\n",
    "\n",
    "# 3. GGUFå½¢å¼ï¼ˆllama.cpp / Ollamaç”¨ï¼‰\n",
    "print(\"\\nğŸ’¾ Saving GGUF (Q4_K_M)...\")\n",
    "model.save_pretrained_gguf(\n",
    "    str(save_path / \"gguf\"),\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",\n",
    ")\n",
    "print(f\"   â†’ {save_path / 'gguf'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ALL MODELS SAVED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ã‚»ãƒ«14: æ¨è«–ãƒ†ã‚¹ãƒˆ\n",
    "# ============================================================\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# æ¨è«–ãƒ¢ãƒ¼ãƒ‰\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate_title(category: str, temperature: float = 0.7):\n",
    "    \"\"\"ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"ã‚ãªãŸã¯note.comã®äººæ°—è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹å°‚é–€AIã§ã™ã€‚èª­è€…ã®èˆˆå‘³ã‚’å¼•ãã€ã‚¯ãƒªãƒƒã‚¯ã—ãŸããªã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚\"},\n",
    "        {\"role\": \"user\", \"content\": f\"ã€Œ{category}ã€ã«é–¢ã™ã‚‹ã€èª­è€…ã‚’æƒ¹ãã¤ã‘ã‚‹noteè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’1ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\"},\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # ç”Ÿæˆéƒ¨åˆ†ã®ã¿æŠ½å‡º\n",
    "    generated = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return generated.strip().split(\"\\n\")[0]\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆ\n",
    "test_categories = [\n",
    "    \"å‰¯æ¥­\",\n",
    "    \"è‹±èªå­¦ç¿’\",\n",
    "    \"ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°\",\n",
    "    \"æŠ•è³‡\",\n",
    "    \"ãƒ–ãƒ­ã‚°é‹å–¶\",\n",
    "    \"AIæ´»ç”¨\",\n",
    "    \"è»¢è·\",\n",
    "    \"ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ GENERATION TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for cat in test_categories:\n",
    "    title = generate_title(cat)\n",
    "    print(f\"\\nã€{cat}ã€‘\")\n",
    "    print(f\"  â†’ {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ã‚»ãƒ«15: è¤‡æ•°ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆï¼ˆãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰\n",
    "# ============================================================\n",
    "\n",
    "def generate_multiple_titles(category: str, n: int = 5):\n",
    "    \"\"\"è¤‡æ•°ã®ã‚¿ã‚¤ãƒˆãƒ«ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ\"\"\"\n",
    "    titles = []\n",
    "    for i in range(n):\n",
    "        # æ¸©åº¦ã‚’å¤‰ãˆã¦ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "        temp = 0.6 + (i * 0.1)\n",
    "        title = generate_title(category, temperature=temp)\n",
    "        titles.append(title)\n",
    "    return titles\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆ\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¨ TITLE VARIATIONS TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for cat in [\"å‰¯æ¥­\", \"AIæ´»ç”¨\"]:\n",
    "    print(f\"\\nã€{cat}ã€‘ã®ã‚¿ã‚¤ãƒˆãƒ«å€™è£œ:\")\n",
    "    print(\"-\" * 40)\n",
    "    titles = generate_multiple_titles(cat, n=5)\n",
    "    for i, title in enumerate(titles, 1):\n",
    "        print(f\"  {i}. {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a86bd70",
   "metadata": {},
   "source": [
    "## âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†\n",
    "\n",
    "### ä¿å­˜å…ˆ\n",
    "- **LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼**: `/content/drive/MyDrive/noteAI/models/{output_name}/lora_adapter/`\n",
    "- **ãƒãƒ¼ã‚¸æ¸ˆã¿16bit**: `/content/drive/MyDrive/noteAI/models/{output_name}/merged_16bit/`\n",
    "- **GGUF (Q4_K_M)**: `/content/drive/MyDrive/noteAI/models/{output_name}/gguf/`\n",
    "\n",
    "### ä½¿ç”¨æ–¹æ³•\n",
    "\n",
    "#### Unslothã§æ¨è«–\n",
    "```python\n",
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"path/to/lora_adapter\")\n",
    "```\n",
    "\n",
    "#### Ollamaã§ä½¿ç”¨\n",
    "```bash\n",
    "ollama create noteai -f Modelfile\n",
    "ollama run noteai \"å‰¯æ¥­ã«é–¢ã™ã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ\"\n",
    "```\n",
    "\n",
    "### æŠ€è¡“ä»•æ§˜ï¼ˆ2026å¹´ä¸–ç•Œæœ€é«˜æ°´æº–ï¼‰\n",
    "| é …ç›® | è¨­å®š | æ ¹æ‹  |\n",
    "|------|------|------|\n",
    "| Framework | Unsloth | 2-5xé«˜é€Ÿã€80%çœVRAM |\n",
    "| Base Model | Qwen2.5-7B-Instruct | æ—¥æœ¬èªæœ€å¼·ã‚¯ãƒ©ã‚¹ |\n",
    "| Method | QLoRA + rsLoRA | é«˜rankå®‰å®šåŒ– |\n",
    "| Target | All Layers | QLoRA-Allæœ€é«˜ç²¾åº¦ |\n",
    "| Training | Completions Only | +1%ç²¾åº¦å‘ä¸Š |\n",
    "| Format | ChatML | Instructå‘ã‘æœ€é© |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
