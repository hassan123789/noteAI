{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b8b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1: ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "# ============================================================\n",
    "!pip install -q transformers accelerate peft bitsandbytes datasets sentencepiece\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cbd579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Google Driveãƒã‚¦ãƒ³ãƒˆï¼†ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "# ============================================================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼‰\n",
    "DATA_PATH = \"/content/drive/MyDrive/noteAI/\"\n",
    "TRAINING_FILE = DATA_PATH + \"generation_training.jsonl\"\n",
    "\n",
    "import json\n",
    "\n",
    "# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "training_data = []\n",
    "with open(TRAINING_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        training_data.append(json.loads(line))\n",
    "\n",
    "print(f\"âœ… å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(training_data)} ä»¶\")\n",
    "print(f\"\\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«:\")\n",
    "print(json.dumps(training_data[0], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94aff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ï¼ˆAlpacaå½¢å¼ï¼‰\n",
    "# ============================================================\n",
    "from datasets import Dataset\n",
    "\n",
    "def format_for_training(example):\n",
    "    \"\"\"Alpacaå½¢å¼ã«å¤‰æ›\"\"\"\n",
    "    text = f\"\"\"### æŒ‡ç¤º:\\n{example['instruction']}\\n\\n### å…¥åŠ›:\\n{example['input'] if example['input'] else 'ï¼ˆãªã—ï¼‰'}\\n\\n### å¿œç­”:\\n{example['output']}\"\"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Datasetä½œæˆ\n",
    "dataset = Dataset.from_list(training_data)\n",
    "dataset = dataset.map(format_for_training)\n",
    "\n",
    "print(f\"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {len(dataset)}\")\n",
    "print(f\"\\nğŸ“„ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¾Œã®ã‚µãƒ³ãƒ—ãƒ«:\")\n",
    "print(\"=\" * 50)\n",
    "print(dataset[0][\"text\"])\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4817b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 4: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆ4bité‡å­åŒ–ï¼‰\n",
    "# ============================================================\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# ğŸ¤– æ—¥æœ¬èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«\n",
    "MODEL_NAME = \"rinna/japanese-gpt-neox-3.6b-instruction-sft\"\n",
    "\n",
    "# é‡å­åŒ–è¨­å®šï¼ˆGPUãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
    "print(f\"ğŸ”„ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­... ({MODEL_NAME})\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "print(f\"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model.num_parameters() / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e641e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 5: LoRAè¨­å®š\n",
    "# ============================================================\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’LoRAå­¦ç¿’ç”¨ã«æº–å‚™\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRAè¨­å®š\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],  # GPT-NeoXç”¨\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# LoRAé©ç”¨\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"âœ… LoRAè¨­å®šå®Œäº†\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac010db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 6: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š\n",
    "# ============================================================\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=DATA_PATH + \"checkpoints\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æº–å‚™å®Œäº†ï¼\")\n",
    "print(f\"ğŸ“Š ã‚¨ãƒãƒƒã‚¯æ•°: {training_args.num_train_epochs}\")\n",
    "print(f\"ğŸ“Š ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"ğŸ“Š å­¦ç¿’ç‡: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba5395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 7: å­¦ç¿’å®Ÿè¡Œ ğŸš€\n",
    "# ============================================================\n",
    "print(\"ğŸš€ å­¦ç¿’é–‹å§‹...\")\n",
    "trainer.train()\n",
    "print(\"\\nâœ… å­¦ç¿’å®Œäº†ï¼\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "MODEL_OUTPUT = DATA_PATH + \"note_title_model\"\n",
    "model.save_pretrained(MODEL_OUTPUT)\n",
    "tokenizer.save_pretrained(MODEL_OUTPUT)\n",
    "print(f\"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {MODEL_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7354f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 8: ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆãƒ†ã‚¹ãƒˆ ğŸ‰\n",
    "# ============================================================\n",
    "\n",
    "def generate_titles(keyword: str, num_titles: int = 5):\n",
    "    \"\"\"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«å€™è£œã‚’ç”Ÿæˆ\"\"\"\n",
    "    prompt = f\"\"\"### æŒ‡ç¤º:\n",
    "ã€Œ{keyword}ã€ã«é–¢ã™ã‚‹ã€èª­è€…ã®èˆˆå‘³ã‚’å¼•ãnoteè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "### å…¥åŠ›:\n",
    "ï¼ˆãªã—ï¼‰\n",
    "\n",
    "### å¿œç­”:\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    titles = []\n",
    "    for i in range(num_titles):\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=60,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "\n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        title = generated.split(\"### å¿œç­”:\")[-1].strip()\n",
    "        # æ”¹è¡Œä»¥é™ã‚’å‰Šé™¤\n",
    "        title = title.split(\"\\n\")[0].strip()\n",
    "        if title and title not in titles:\n",
    "            titles.append(title)\n",
    "\n",
    "    return titles\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¯ ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆãƒ†ã‚¹ãƒˆ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_keywords = [\"å‰¯æ¥­ã§æœˆ5ä¸‡å††\", \"AIæ´»ç”¨è¡“\", \"30ä»£ã®è»¢è·\", \"å­è‚²ã¦ã®æ‚©ã¿\"]\n",
    "\n",
    "for kw in test_keywords:\n",
    "    print(f\"\\nğŸ“ ã€{kw}ã€‘ã®ã‚¿ã‚¤ãƒˆãƒ«æ¡ˆ:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, title in enumerate(generate_titles(kw), 1):\n",
    "        print(f\"  {i}. {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64830d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 9: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ç”Ÿæˆ\n",
    "# ============================================================\n",
    "\n",
    "# è‡ªç”±ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ\n",
    "keyword = input(\"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›: \")\n",
    "print(f\"\\nğŸ¯ ã€{keyword}ã€‘ã®ã‚¿ã‚¤ãƒˆãƒ«æ¡ˆ:\")\n",
    "for i, title in enumerate(generate_titles(keyword, 10), 1):\n",
    "    print(f\"  {i}. {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b126431a",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã¯Google Driveä¸Šã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™:\n",
    "- `MyDrive/noteAI/note_title_model/`\n",
    "\n",
    "ãƒ­ãƒ¼ã‚«ãƒ«ã§ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ä¸Šè¨˜ãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
